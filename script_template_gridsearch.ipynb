{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge SD207 - 2017\n",
    "*<p>Author: Pengfei MI, Rui SONG</p>*\n",
    "*<p>Date: 06/06/2017</p>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "from time import time\n",
    "\n",
    "# Librosa related: audio feature extraction\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Sklearn related: data preprocessing and classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some usefull functions\n",
    "def load_sound_file(file_name):\n",
    "    X, sr = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    return X\n",
    "\n",
    "def extract_feature(file_name, feature_type): # Late fusion\n",
    "    X, sample_rate = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    if feature_type == 'mfcc':\n",
    "        return librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "    elif feature_type == 'mfcc_0':\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)[1:].T\n",
    "        return mfcc\n",
    "    elif feature_type == \"d_mfcc\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        return librosa.feature.delta(mfcc, width=width, order=1, trim=True)\n",
    "    elif feature_type == \"dd_mfcc\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        return librosa.feature.delta(mfcc, width=width, order=2, trim=True)\n",
    "    elif feature_type == \"mfcc_d\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        d_mfcc = librosa.feature.delta(mfcc, width=width, order=1, trim=True)\n",
    "        return np.c_[mfcc, d_mfcc]\n",
    "    elif feature_type == \"mfcc_dd\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        d_mfcc = librosa.feature.delta(mfcc, width=width, order=1, trim=True)\n",
    "        dd_mfcc = librosa.feature.delta(mfcc, width=width, order=2, trim=True)\n",
    "        return np.c_[mfcc, d_mfcc, dd_mfcc]\n",
    "    \n",
    "def parse_audio_files(file_names, file_labels, feature_type):\n",
    "    # Try to detect the feature dimension\n",
    "    n_features = extract_feature(file_names[0], feature_type).shape[1]\n",
    "    features, labels = np.empty((0, n_features)), np.empty(0)\n",
    "    for fn, fl in zip(file_names, file_labels):\n",
    "        ff = extract_feature(fn, feature_type)\n",
    "        features = np.vstack([features, ff])\n",
    "        labels = np.append(labels, fl*np.ones(ff.shape[0]))\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "def cross_validation(clf, X, y, test_fold, feature_type=\"mfcc\"):\n",
    "    y_pred, y_pred_sum, y_pred_prod = np.empty_like(y), np.empty_like(y), np.empty_like(y)\n",
    "    n_folds = len(np.unique(test_fold))\n",
    "    for i in range(n_folds):\n",
    "        t0 = time()\n",
    "        new_clf = clone(clf, safe=True)\n",
    "        X_train = X[test_fold != i]\n",
    "        X_test = X[test_fold == i]\n",
    "        y_train = y[test_fold != i]\n",
    "        y_test = y[test_fold == i]\n",
    "        print \"Launching fold #%d/%d, train set size: %d, test set size: %d\" % (i+1, n_folds, len(X_train), len(X_test))\n",
    "        clf_train(new_clf, X_train, y_train, feature_type)\n",
    "        test_pred, test_pred_sum, test_pred_prod = clf_predict(new_clf, X_test, feature_type)\n",
    "        y_pred[test_fold == i] = test_pred\n",
    "        y_pred_sum[test_fold == i] = test_pred_sum\n",
    "        y_pred_prod[test_fold == i] = test_pred_prod\n",
    "        print \"fold#%d done in %0.3fs, score: %0.3f.\" % (i+1, time()-t0, accuracy_score(y_test, test_pred))\n",
    "    t0 = time()\n",
    "    print \"Retraining classifier with whole train set...\"\n",
    "    clf_train(clf, X, y, feature_type)\n",
    "    print \"Done in %0.3fs.\" % (time() - t0)\n",
    "    return y_pred, y_pred_sum, y_pred_prod\n",
    "\n",
    "def clf_train(clf, files, file_labels, feature_type):\n",
    "    X_train, y_train= parse_audio_files(files, file_labels, feature_type)\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "def predict_maj(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_pred = np.append(y_pred, sp.stats.mode(y_predicts).mode[0])\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_sum(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = np.sum(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_prod(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = np.prod(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def clf_predict(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    y_pred_sum = np.empty(0)\n",
    "    y_pred_prod = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_predict_probas = clf.predict_proba(x_mfccs)\n",
    "        y_pred = np.append(y_pred, sp.stats.mode(y_predicts).mode[0])\n",
    "        y_pred_sum = np.append(y_pred_sum, np.argmax(np.sum(y_predict_probas, axis=0)))\n",
    "        y_pred_prod = np.append(y_pred_prod, np.argmax(np.prod(y_predict_probas, axis=0)))\n",
    "    return np.array(y_pred, dtype=np.int), np.array(y_pred_sum, dtype=np.int), np.array(y_pred_prod, dtype=np.int)\n",
    "\n",
    "class AcousticSceneClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Classifier adapted to acoustic scene classification.\"\"\"\n",
    "    def __init__(self, feature_type=\"mfcc\", n_mfcc=20, n_fft=512, hop_length=512, width=3, n_fusion=-1, \\\n",
    "                 hidden_layer_sizes=(100), alpha=0.0001, learning_rate_init=0.001):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "        self.features_ = {}\n",
    "        self.clf_ = MLPClassifier(hidden_layer_sizes=self.hidden_layer_sizes, alpha=self.alpha, learning_rate_init=self.learning_rate_init)\n",
    "        \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        self.features_ = {}\n",
    "        self.clf_.set_params(**{'hidden_layer_sizes': self.hidden_layer_sizes,\n",
    "                                'alpha': self.alpha,\n",
    "                                'learning_rate_init': self.learning_rate_init})\n",
    "        return self\n",
    "    \n",
    "    def _extract_feature(self, file_name): # Late fusion\n",
    "        if file_name not in self.features_:\n",
    "            X, sample_rate = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "            frame_mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=self.n_fft, hop_length=self.hop_length, n_mfcc=self.n_mfcc).T\n",
    "            if self.feature_type == 'mfcc':\n",
    "                frame_feature = frame_mfcc\n",
    "            elif self.feature_type == 'mfcc_0':\n",
    "                frame_feature = frame_mfcc[:, 1:]\n",
    "            elif self.feature_type == \"d_mfcc\":\n",
    "                frame_feature = librosa.feature.delta(frame_mfcc, width=width, order=1, trim=True)\n",
    "            elif self.feature_type == \"dd_mfcc\":\n",
    "                frame_feature = librosa.feature.delta(frame_mfcc, width=width, order=2, trim=True)\n",
    "            elif self.feature_type == \"mfcc_d\":\n",
    "                d_mfcc = librosa.feature.delta(frame_mfcc, width=width, order=1, trim=True)\n",
    "                frame_feature = np.c_[frame_mfcc, d_mfcc]\n",
    "            else:\n",
    "                d_mfcc = librosa.feature.delta(mfcc, width=width, order=1, trim=True)\n",
    "                dd_mfcc = librosa.feature.delta(mfcc, width=width, order=2, trim=True)\n",
    "                frame_feature = np.c_[mfcc, d_mfcc, dd_mfcc]\n",
    "\n",
    "            if self.n_fusion == -1:\n",
    "                self.features_[file_name] = frame_feature\n",
    "            elif self.n_fusion == 0:\n",
    "                self.features_[file_name] = np.mean(frame_feature, axis=0).reshape((1, -1))\n",
    "            else:\n",
    "                n_frames = frame_feature.shape[0]/n_fusion\n",
    "                fusion_mean = np.empty((n_frames, frame_feature.shape[1]))\n",
    "                #fusion_var = fusion_mean = np.empty((n_frames, frame_feature.shape[1]))\n",
    "                for i in range(n_frames):\n",
    "                    fusion_mean[i, :] = np.mean(frame_feature[n_fusion*i:n_fusion*(i+1)-1, :], axis=0)\n",
    "                    #fusion_var[i, :] = np.std(frame_feature[n_fusion*i:n_fusion*(i+1)-1, :], axis=0)\n",
    "                #return np.c_[fusion_mean, fusion_var]\n",
    "                self.features_[file_name] = fusion_mean\n",
    "        return self.features_[file_name]\n",
    "\n",
    "    def _parse_audio_files(self, file_names, file_labels):\n",
    "        # Try to detect the feature dimension\n",
    "        n_features = self._extract_feature(file_names[0]).shape[1]\n",
    "        features, labels = np.empty((0, n_features)), np.empty(0)\n",
    "        for fn, fl in zip(file_names, file_labels):\n",
    "            ff = self._extract_feature(fn)\n",
    "            features = np.vstack([features, ff])\n",
    "            labels = np.append(labels, fl*np.ones(ff.shape[0]))\n",
    "        return np.array(features), np.array(labels, dtype = np.int)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_train, y_train= self._parse_audio_files(X, y)\n",
    "        self.clf_.fit(X_train, y_train)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X, rule=\"maj\"):\n",
    "        y_pred = np.empty(0)\n",
    "        for x in X:\n",
    "            x_feature = self._extract_feature(x)\n",
    "            if rule == \"maj\":\n",
    "                y_predicts = self.clf_.predict(x_feature)\n",
    "                y_pred = np.append(y_pred, sp.stats.mode(y_predicts).mode[0])\n",
    "            elif rule == \"sum\":\n",
    "                y_predict_probas = self.clf_.predict_proba(x_feature)\n",
    "                y_pred = np.append(y_pred, np.argmax(np.sum(y_predict_probas, axis=0)))\n",
    "            else:\n",
    "                y_predict_probas = self.clf_.predict_proba(x_feature)\n",
    "                y_pred = np.append(y_pred, np.argmax(np.prod(y_predict_probas, axis=0)))\n",
    "        return np.array(y_pred, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Fold #1: 290 files from 45 sources\n",
      "Fold #2: 292 files from 43 sources\n",
      "Fold #3: 290 files from 45 sources\n",
      "15 scenes: ['beach' 'bus' 'cafe/restaurant' 'car' 'city_center' 'forest_path'\n",
      " 'grocery_store' 'home' 'library' 'metro_station' 'office' 'park'\n",
      " 'residential_area' 'train' 'tram']\n",
      "Training set size: 872\n",
      "Test set size: 298\n",
      "Done in 0.017s.\n"
     ]
    }
   ],
   "source": [
    "# Read data and preprocessing\n",
    "print \"Loading files...\"\n",
    "t0 = time()\n",
    "\n",
    "# Define FILEROOT according to the platform\n",
    "# My personal computer\n",
    "if sys.platform == \"darwin\":\n",
    "    FILEROOT = './'\n",
    "# Node of Telecom\n",
    "elif platform.node()[:4] == 'lame':\n",
    "    FILEROOT = '/tmp/'\n",
    "# The machines of Telecom\n",
    "else:\n",
    "    FILEROOT = '/tsi/plato/sons/sd207/'\n",
    "\n",
    "# Load the cross validation folds\n",
    "N_FOLDS = 3\n",
    "train_files, train_scenes, test_fold = np.empty(0, dtype=str), np.empty(0), np.empty(0)\n",
    "for i in range(N_FOLDS):\n",
    "    files = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[0].values\n",
    "    scenes = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[1].values\n",
    "    print \"Fold #%d: %d files from %d sources\" % (i+1, len(files), len(np.unique([f.split('_')[0] for f in files])))\n",
    "    train_files = np.append(train_files, files)\n",
    "    train_scenes = np.append(train_scenes, scenes)\n",
    "    test_fold = np.append(test_fold, i*np.ones_like(scenes))\n",
    "\n",
    "scenes = np.unique(train_scenes)\n",
    "n_scenes = len(scenes)\n",
    "labels = pd.factorize(scenes, sort=True)[0]\n",
    "n_labels = len(labels)\n",
    "train_labels = pd.factorize(train_scenes, sort=True)[0]\n",
    "test_files = pd.read_csv('test_files.txt', header=None)[0].values\n",
    "test_labels = pd.read_csv('meta.txt', header=None)[0].values\n",
    "\n",
    "print \"%d scenes:\" % n_scenes, scenes\n",
    "print \"Training set size: %d\" % len(train_files)\n",
    "print \"Test set size: %d\" % len(test_files)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing cross validation...\n",
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "[CV] hop_length=4096, n_fft=4096 .....................................\n",
      "[CV] hop_length=4096, n_fft=2048 .....................................\n",
      "[CV] hop_length=4096, n_fft=4096 .....................................\n",
      "[CV] hop_length=4096, n_fft=4096 .....................................\n",
      "[CV] hop_length=4096, n_fft=4096 .....................................\n",
      "[CV] hop_length=4096, n_fft=2048 .....................................\n",
      "[CV] hop_length=4096, n_fft=2048 .....................................\n",
      "[CV] hop_length=4096, n_fft=2048 .....................................\n",
      "[CV] ...... hop_length=4096, n_fft=2048, score=0.755656, total= 1.3min\n",
      "[CV] hop_length=2048, n_fft=4096 .....................................\n",
      "[CV] ...... hop_length=4096, n_fft=2048, score=0.779817, total= 1.7min\n",
      "[CV] hop_length=2048, n_fft=4096 .....................................\n",
      "[CV] ...... hop_length=4096, n_fft=2048, score=0.671171, total= 1.8min\n",
      "[CV] hop_length=2048, n_fft=4096 .....................................\n",
      "[CV] ...... hop_length=4096, n_fft=4096, score=0.734234, total= 2.3min\n",
      "[CV] hop_length=2048, n_fft=4096 .....................................\n",
      "[CV] ...... hop_length=4096, n_fft=4096, score=0.559242, total= 2.3min\n",
      "[CV] hop_length=2048, n_fft=2048 .....................................\n",
      "[CV] ...... hop_length=4096, n_fft=4096, score=0.782805, total= 2.3min\n",
      "[CV] hop_length=2048, n_fft=2048 .....................................\n",
      "[CV] ...... hop_length=4096, n_fft=4096, score=0.788991, total= 2.5min\n",
      "[CV] hop_length=2048, n_fft=2048 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  16 | elapsed:  2.6min remaining:  3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... hop_length=4096, n_fft=2048, score=0.616114, total= 2.8min\n",
      "[CV] hop_length=2048, n_fft=2048 .....................................\n",
      "[CV] ...... hop_length=2048, n_fft=4096, score=0.796380, total= 4.2min\n",
      "[CV] ...... hop_length=2048, n_fft=4096, score=0.825688, total= 4.2min\n",
      "[CV] ...... hop_length=2048, n_fft=4096, score=0.616114, total= 4.2min\n",
      "[CV] ...... hop_length=2048, n_fft=4096, score=0.729730, total= 5.3min\n",
      "[CV] ...... hop_length=2048, n_fft=2048, score=0.791855, total= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  13 out of  16 | elapsed:  6.9min remaining:  1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... hop_length=2048, n_fft=2048, score=0.611374, total= 4.1min\n",
      "[CV] ...... hop_length=2048, n_fft=2048, score=0.716216, total= 4.8min\n",
      "[CV] ...... hop_length=2048, n_fft=2048, score=0.821101, total= 4.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AcousticSceneClassifier(alpha=0.1, feature_type='mfcc', hidden_layer_sizes=40,\n",
      "            hop_length=2048, learning_rate_init=0.001, n_fft=4096,\n",
      "            n_fusion=-1, n_mfcc=40, width=3)\n",
      "Done in 538.626s.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "print \"Doing cross validation...\"\n",
    "t0 = time()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "params = [{'feature_type': \"mfcc\",\n",
    "           'n_mfcc': [17, 20, 25, 40],\n",
    "           'n_fft': [512, 1024, 2048, 4096],\n",
    "           'hop_length': [512, 1024, 2048, 40],\n",
    "           'n_fusion': [-1, 0, 3, 5]\n",
    "           'hidden_layer_sizes': [(40), (40, 20), (40, 80, 40), (256), (256, 256, 256)],\n",
    "           'alpha': np.logspace(-5, 3, 7)}]\n",
    "\n",
    "asc = AcousticSceneClassifier(n_fusion=-1, hidden_layer_sizes=(40), alpha=0.1)\n",
    "clf = GridSearchCV(asc, params, cv=4, n_jobs=-1, verbose=3)\n",
    "clf.fit(train_files, train_labels)\n",
    "\n",
    "print clf.best_estimator_\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf.best_estimator_.predict(test_files)\n",
    "y_pred_sum = clf.best_estimator_.predict(test_files, \"sum\")\n",
    "y_pred_prod = clf.best_estimator_.predict(test_files, \"prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"# Print cross validation results\n",
    "t0 = time()\n",
    "print '-'*60\n",
    "print \"Score on validation test (vote by majority): %f\" % accuracy_score(train_labels, y_pred)\n",
    "print classification_report(train_labels, y_pred, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred)\n",
    "\n",
    "print '-'*60\n",
    "print \"Score on validation test (vote by proba sum): %f\" % accuracy_score(train_labels, y_pred_sum )\n",
    "print classification_report(train_labels, y_pred_sum, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_sum)\n",
    "\n",
    "print '-'*60\n",
    "print \"Score on validation test (vote by proba product): %f\" % accuracy_score(train_labels, y_pred_prod)\n",
    "print classification_report(train_labels, y_pred_prod, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_prod)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score by maj: 0.711409\n",
      "Score by sum: 0.708054\n",
      "Score by prod: 0.734899\n"
     ]
    }
   ],
   "source": [
    "#y_test_pred, y_test_pred_sum, y_test_pred_prod = clf_predict(clf, test_files, feature_type)\n",
    "\n",
    "print \"Score by maj: %f\" % accuracy_score(test_labels, y_pred)\n",
    "print \"Score by sum: %f\" % accuracy_score(test_labels, y_pred_sum)\n",
    "print \"Score by prod: %f\" % accuracy_score(test_labels, y_pred_prod)\n",
    "\n",
    "np.savetxt('y_test_pred_mfcc_mlp_gs.txt', y_test_pred, fmt='%d')\n",
    "np.savetxt('y_test_pred_mfcc_mlp_gs_sum.txt', y_test_pred_sum, fmt='%d')\n",
    "np.savetxt('y_test_pred_mfcc_mlp_gs_prod.txt', y_test_pred_prod, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
