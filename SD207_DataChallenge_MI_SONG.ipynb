{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge SD207 - 2017\n",
    "*<p>Author: Pengfei MI, Rui SONG</p>*\n",
    "*<p>Date: 06/06/2017</p>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import platform\n",
    "import os\n",
    "from time import time\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Librosa related: audio feature extraction\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Sklearn related: data preprocessing and classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some usefull functions\n",
    "def load_sound_file(file_name):\n",
    "    X, sr = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    return X\n",
    "\n",
    "def extract_feature(file_name): # Late fusion\n",
    "    X, sample_rate = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=4096, hop_length=2048, n_mfcc=n_mfcc).T\n",
    "    #delta_mfcc = librosa.feature.delta(mfcc, width=5, order=1, trim=True)\n",
    "    return mfcc\n",
    "\n",
    "def parse_audio_files(file_names, file_labels, test_fold):\n",
    "    features, labels, tf = np.empty((0,n_features)), np.empty(0), np.empty(0)\n",
    "    for fn, fl, f in zip(file_names, file_labels, test_fold):\n",
    "        try:\n",
    "            ff = extract_feature(fn)\n",
    "        except Exception as e:\n",
    "            print \"Error encountered while parsing file: \", fn\n",
    "            continue\n",
    "        features = np.vstack([features, ff])\n",
    "        labels = np.append(labels, fl*np.ones(ff.shape[0]))\n",
    "        tf = np.append(tf, f*np.ones(ff.shape[0]))\n",
    "    return np.array(features), np.array(labels, dtype = np.int), np.array(tf, dtype=np.int)\n",
    "\n",
    "def predict(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    y_pred_sum = np.empty(0)\n",
    "    y_pred_prod = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_predict_probas = clf.predict_proba(x_mfccs)\n",
    "        y_pred = np.append(y_pred, mode(y_predicts).mode[0])\n",
    "        y_pred_sum = np.append(y_pred_sum, np.argmax(np.sum(y_predict_probas, axis=0)))\n",
    "        y_pred_prod = np.append(y_pred_prod, np.argmax(np.prod(y_predict_probas, axis=0)))\n",
    "    return np.array(y_pred, dtype=np.int), np.array(y_pred_sum, dtype=np.int), np.array(y_pred_prod, dtype=np.int)\n",
    "\n",
    "def predict_maj(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_pred = np.append(y_pred, mode(y_predicts).mode[0])\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_sum(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = np.sum(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_prod(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = np.prod(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Fold #0: 194 files from 29 sources\n",
      "Fold #1: 194 files from 29 sources\n",
      "Fold #2: 194 files from 30 sources\n",
      "Training set size: 582\n",
      "Validation set size: 290\n",
      "Test set size: 298\n",
      "Done in 0.021s.\n"
     ]
    }
   ],
   "source": [
    "# Read data and preprocessing\n",
    "print \"Loading files...\"\n",
    "t0 = time()\n",
    "\n",
    "# Define FILEROOT according to the platform\n",
    "# My personal computer\n",
    "if platform == \"darwin\":\n",
    "    FILEROOT = './'\n",
    "# The machines of Telecom\n",
    "else:\n",
    "    FILEROOT = '/tsi/plato/sons/sd207/'\n",
    "\n",
    "# Load the cross validation folds\n",
    "N_FOLDS = 3\n",
    "train_files, train_scenes, test_fold = np.empty(0, dtype=str), np.empty(0), np.empty(0)\n",
    "for i in range(N_FOLDS):\n",
    "    files = pd.read_csv('fold%s.txt' % str(i), sep='\\s+', header=None)[0].values\n",
    "    scenes = pd.read_csv('fold%s.txt' % str(i), sep='\\s+', header=None)[1].values\n",
    "    print \"Fold #%d: %d files from %d sources\" % (i, len(files), len(np.unique([f.split('_')[0] for f in files])))\n",
    "    train_files = np.append(train_files, files)\n",
    "    train_scenes = np.append(train_scenes, scenes)\n",
    "    test_fold = np.append(test_fold, i*np.ones_like(scenes))\n",
    "\n",
    "scenes = np.unique(train_scenes)\n",
    "n_scenes = len(scenes)\n",
    "labels = pd.factorize(scenes)[0]\n",
    "n_labels = len(labels)\n",
    "train_labels = pd.factorize(train_scenes)[0]\n",
    "val_files = pd.read_csv('dev.txt', header=None)[0].values\n",
    "val_scenes = pd.read_csv('dev.txt', sep='\\s+', header=None)[1].values\n",
    "val_labels = pd.factorize(val_scenes)[0]\n",
    "test_files = pd.read_csv('test_files.txt', header=None)[0].values\n",
    "\n",
    "print \"Training set size: %d\" % len(train_files)\n",
    "print \"Validation set size: %d\" % len(val_files)\n",
    "print \"Test set size: %d\" % len(test_files)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "print \"Extracting features...\"\n",
    "t0 = time()\n",
    "\n",
    "n_mfcc = 40\n",
    "n_features = 40\n",
    "X_train, y_train, test_fold_train = parse_audio_files(train_files, train_labels, test_fold)\n",
    "ps = PredefinedSplit(test_fold_train)\n",
    "print X_train.shape, y_train.shape, test_fold_train.shape\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "print \"Training classifier...\"\n",
    "np.random.seed(42)\n",
    "t0 = time()\n",
    "\n",
    "eatimator = \n",
    "params = {\n",
    "          }\n",
    "clf = GridSearchCV(eatimator, params, cv=ps, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predicting on validation set...\n",
    "t0 = time()\n",
    "y_pred, y_pred_sum, y_pred_prod = predict(clf, val_files)\n",
    "print \"Score on validation test (vote by majority): %f\" % np.mean(y_pred == val_labels)\n",
    "print classification_report(val_labels, y_pred, target_names=labels)\n",
    "\n",
    "print \"Score on validation test (vote by proba sum): %f\" % np.mean(y_pred_sum == val_labels)\n",
    "print classification_report(val_labels, y_pred_sum, target_names=labels)\n",
    "\n",
    "print \"Score on validation test (vote by proba product): %f\" % np.mean(y_pred_prod == val_labels)\n",
    "print classification_report(val_labels, y_pred_prod, target_names=scenes)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred, y_test_pred_sum, y_test_pred_prod = predict(clf, test_files)\n",
    "np.savetxt('y_test_pred_delta_mfcc_mlp.txt', y_test_pred, fmt='%d')\n",
    "np.savetxt('y_test_pred_delta_mfcc_mlp_sum.txt', y_test_pred_sum, fmt='%d')\n",
    "np.savetxt('y_test_pred_delta_mfcc_mlp_prod.txt', y_test_pred_prod, fmt='%d')\n",
    "print y_test_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
