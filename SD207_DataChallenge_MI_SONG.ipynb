{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge SD207 - 2017\n",
    "*<p>Author: Pengfei MI, Rui SONG</p>*\n",
    "*<p>Date: 06/06/2017</p>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import platform\n",
    "import os\n",
    "import platform\n",
    "from time import time\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Librosa related: audio feature extraction\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Sklearn related: data preprocessing and classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some usefull functions\n",
    "def load_sound_file(file_name):\n",
    "    X, sr = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    return X\n",
    "\n",
    "def extract_feature(file_name): # Late fusion\n",
    "    if file_name not in file_features:\n",
    "        X, sample_rate = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        #delta_mfcc = librosa.feature.delta(mfcc, width=5, order=1, trim=True)\n",
    "        file_features[file_name] = mfcc\n",
    "    return file_features[file_name]\n",
    "\n",
    "def parse_audio_files(file_names, file_labels):\n",
    "    features, labels = np.empty((0,n_features)), np.empty(0)\n",
    "    for fn, fl in zip(file_names, file_labels):\n",
    "        try:\n",
    "            ff = extract_feature(fn)\n",
    "        except Exception as e:\n",
    "            print \"Error encountered while parsing file: \", fn\n",
    "            continue\n",
    "        features = np.vstack([features, ff])\n",
    "        labels = np.append(labels, fl*np.ones(ff.shape[0]))\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "def cross_validation(clf, X, y, test_fold):\n",
    "    y_pred, y_pred_sum, y_pred_prod = np.empty_like(y), np.empty_like(y), np.empty_like(y)\n",
    "    n_folds = len(np.unique(test_fold))\n",
    "    for i in range(n_folds):\n",
    "        t0 = time()\n",
    "        new_clf = clone(clf, safe=True)\n",
    "        X_train = X[test_fold != i]\n",
    "        X_test = X[test_fold == i]\n",
    "        y_train = y[test_fold != i]\n",
    "        y_test = y[test_fold == i]\n",
    "        print \"Launching fold #%d/%d, train set size: %d, test set size: %d\" % (i+1, n_folds, len(X_train), len(X_test))\n",
    "        clf_train(new_clf, X_train, y_train)\n",
    "        test_pred, test_pred_sum, test_pred_prod = clf_predict(new_clf, X_test)\n",
    "        y_pred[test_fold == i] = test_pred\n",
    "        y_pred_sum[test_fold == i] = test_pred_sum\n",
    "        y_pred_prod[test_fold == i] = test_pred_prod\n",
    "        print \"fold#%d done in %0.3fs, score: %0.3f.\" % (i+1, time()-t0, accuracy_score(y_test, test_pred))\n",
    "    t0 = time()\n",
    "    print \"Retraining classifier with whole train set...\"\n",
    "    clf_train(clf, X, y)\n",
    "    print \"Done in %0.3fs.\" % (time() - t0)\n",
    "    return y_pred, y_pred_sum, y_pred_prod\n",
    "\n",
    "def clf_train(clf, files, file_labels):\n",
    "    X_train, y_train= parse_audio_files(files, file_labels)\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "def predict_maj(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_pred = np.append(y_pred, mode(y_predicts).mode[0])\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_sum(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = np.sum(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_prod(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = np.prod(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def clf_predict(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    y_pred_sum = np.empty(0)\n",
    "    y_pred_prod = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_predict_probas = clf.predict_proba(x_mfccs)\n",
    "        y_pred = np.append(y_pred, mode(y_predicts).mode[0])\n",
    "        y_pred_sum = np.append(y_pred_sum, np.argmax(np.sum(y_predict_probas, axis=0)))\n",
    "        y_pred_prod = np.append(y_pred_prod, np.argmax(np.prod(y_predict_probas, axis=0)))\n",
    "    return np.array(y_pred, dtype=np.int), np.array(y_pred_sum, dtype=np.int), np.array(y_pred_prod, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data and preprocessing\n",
    "print \"Loading files...\"\n",
    "t0 = time()\n",
    "\n",
    "# Define FILEROOT according to the platform\n",
    "# My personal computer\n",
    "if platform == \"darwin\":\n",
    "    FILEROOT = './'\n",
    "# Node of Telecom\n",
    "elif platform.node()[:4] == 'lame'\n",
    "    FILEROOT = '/tmp/rsong/'\n",
    "# The machines of Telecom\n",
    "else:\n",
    "    FILEROOT = '/tsi/plato/sons/sd207/'\n",
    "\n",
    "# Load the cross validation folds\n",
    "N_FOLDS = 3\n",
    "train_files, train_scenes, test_fold = np.empty(0, dtype=str), np.empty(0), np.empty(0)\n",
    "for i in range(N_FOLDS):\n",
    "    files = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[0].values\n",
    "    scenes = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[1].values\n",
    "    print \"Fold #%d: %d files from %d sources\" % (i+1, len(files), len(np.unique([f.split('_')[0] for f in files])))\n",
    "    train_files = np.append(train_files, files)\n",
    "    train_scenes = np.append(train_scenes, scenes)\n",
    "    test_fold = np.append(test_fold, i*np.ones_like(scenes))\n",
    "\n",
    "scenes = np.unique(train_scenes)\n",
    "n_scenes = len(scenes)\n",
    "labels = pd.factorize(scenes, sort=True)[0]\n",
    "n_labels = len(labels)\n",
    "train_labels = pd.factorize(train_scenes, sort=True)[0]\n",
    "test_files = pd.read_csv('test_files.txt', header=None)[0].values\n",
    "test_labels = pd.read_csv('meta.txt', header=None)[0].values\n",
    "\n",
    "print \"%d scenes:\" % n_scenes, scenes\n",
    "print \"Training set size: %d\" % len(train_files)\n",
    "print \"Test set size: %d\" % len(test_files)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "print \"Doing cross validation...\"\n",
    "t0 = time()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_mfcc = 20\n",
    "n_fft = 1024\n",
    "hop_length = 512\n",
    "n_features = n_mfcc\n",
    "file_features = {}\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(128, 64), alpha=0.4)\n",
    "y_pred, y_pred_sum, y_pred_prod = cross_validation(clf, train_files, train_labels, test_fold)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)\n",
    "\n",
    "\n",
    "\"\"\"param_grid = {'n_mfcc': [17, 20, 40],\n",
    "              'n_fft': [512, 1024],\n",
    "              'hop_length': [512],\n",
    "              'hidden_layer_sizes': [(40), (40,80), (40,20), (64,64,64)],\n",
    "              'alpha': [0.001, 0.01, 0.05, 0.1]}\n",
    "params = list(ParameterGrid(param_grid))\n",
    "\n",
    "best_score, best_score_sum, best_score_prod = 0, 0, 0\n",
    "best_param, best_param_sum, best_param_prod = None, None, None\n",
    "\n",
    "total_fits = len(params)\n",
    "for i, param in enumerate(params):\n",
    "    n_mfcc = param['n_mfcc']\n",
    "    n_fft = param['n_fft']\n",
    "    hop_length = param['hop_length']\n",
    "    n_features = n_mfcc\n",
    "    file_features = {}\n",
    "    sizes = param['hidden_layer_sizes']\n",
    "    alpha = param['alpha']\n",
    "\n",
    "    print \"Launch fit #%d/%d with: n_mfcc=%d, n_fft=%d, hop_length=%d, nn_sizes=%s, alpha=%f\" % \\\n",
    "                   (i, total_fits, n_mfcc, n_fft, hop_length, sizes, alpha)\n",
    "    clf = MLPClassifier(hidden_layer_sizes=sizes, alpha=alpha)\n",
    "    y_pred, y_pred_sum, y_pred_prod = cross_validation(clf, train_files, train_labels, test_fold)\n",
    "    if (accuracy_score(train_labels,y_pred) > best_score):\n",
    "        best_score = accuracy_score(train_labels,y_pred)\n",
    "        best_param = p.copy()\n",
    "    if (accuracy_score(train_labels,y_pred_sum) > best_score_sum):\n",
    "        best_score_sum = accuracy_score(train_labels,y_pred_sum)\n",
    "        best_param_sum = p.copy()\n",
    "    if (accuracy_score(train_labels,y_pred_prod) > best_score_prod):\n",
    "        best_score_prod = accuracy_score(train_labels,y_pred_prod)\n",
    "        best_param_prod = p.copy()\n",
    "print best_score, best_param\n",
    "print best_score_sum, best_param_sum\n",
    "print best_score_prod, best_param_prod\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print cross validation results\n",
    "t0 = time()\n",
    "print '-'*60\n",
    "print \"Score on validation test (vote by majority): %f\" % accuracy_score(train_labels, y_pred)\n",
    "print classification_report(train_labels, y_pred, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred)\n",
    "\n",
    "print '-'*60\n",
    "print \"Score on validation test (vote by proba sum): %f\" % accuracy_score(train_labels, y_pred_sum )\n",
    "print classification_report(train_labels, y_pred_sum, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_sum)\n",
    "\n",
    "print '-'*60\n",
    "print \"Score on validation test (vote by proba product): %f\" % accuracy_score(train_labels, y_pred_prod)\n",
    "print classification_report(train_labels, y_pred_prod, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_prod)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred, y_test_pred_sum, y_test_pred_prod = clf_predict(clf, test_files)\n",
    "np.savetxt('y_test_pred_mfcc_mlp.txt', y_test_pred, fmt='%d')\n",
    "np.savetxt('y_test_pred_mfcc_mlp_sum.txt', y_test_pred_sum, fmt='%d')\n",
    "np.savetxt('y_test_pred_mfcc_mlp_prod.txt', y_test_pred_prod, fmt='%d')\n",
    "\n",
    "print \"Score by maj: %f\" % accuracy_score(test_labels, y_test_pred)\n",
    "print \"Score by sum: %f\" % accuracy_score(test_labels, y_test_pred_sum)\n",
    "print \"Score by prod: %f\" % accuracy_score(test_labels, y_test_pred_prod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
