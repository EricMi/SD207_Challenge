{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge SD207 - 2017\n",
    "*<p>Author: Pengfei MI, Rui SONG</p>*\n",
    "*<p>Date: 06/06/2017</p>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import platform\n",
    "import os\n",
    "from time import time\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Librosa related: audio feature extraction\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Sklearn related: data preprocessing and classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some usefull functions\n",
    "def load_sound_file(file_name):\n",
    "    X, sr = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    return X\n",
    "\n",
    "def extract_feature(file_name): # Late fusion\n",
    "    if file_name not in file_features:\n",
    "        X, sample_rate = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=4096, hop_length=2048, n_mfcc=n_mfcc).T\n",
    "        #delta_mfcc = librosa.feature.delta(mfcc, width=5, order=1, trim=True)\n",
    "        file_features[file_name] = mfcc\n",
    "    return file_features[file_name]\n",
    "\n",
    "def parse_audio_files(file_names, file_labels):\n",
    "    features, labels = np.empty((0,n_features)), np.empty(0)\n",
    "    for fn, fl in zip(file_names, file_labels):\n",
    "        try:\n",
    "            ff = extract_feature(fn)\n",
    "        except Exception as e:\n",
    "            print \"Error encountered while parsing file: \", fn\n",
    "            continue\n",
    "        features = np.vstack([features, ff])\n",
    "        labels = np.append(labels, fl*np.ones(ff.shape[0]))\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "def cross_validation(clf, X, y, test_fold):\n",
    "    y_pred, y_pred_sum, y_pred_prod = np.empty_like(y), np.empty_like(y), np.empty_like(y)\n",
    "    n_folds = len(np.unique(test_fold))\n",
    "    for i in range(n_folds):\n",
    "        print \"Launching fold #%d/%d\" % (i+1, n_folds)\n",
    "        t0 = time()\n",
    "        new_clf = clone(clf, safe=True)\n",
    "        X_train = X[test_fold != i]\n",
    "        X_test = X[test_fold == i]\n",
    "        y_train = y[test_fold != i]\n",
    "        y_test = y[test_fold == i]\n",
    "        print \"train set size: %d, test set size: %d\" % (len(X_train), len(X_test))\n",
    "        clf_train(new_clf, X_train, y_train)\n",
    "        test_pred, test_pred_sum, test_pred_prod = clf_predict(new_clf, X_test)\n",
    "        y_pred[test_fold == i] = test_pred\n",
    "        y_pred_sum[test_fold == i] = test_pred_sum\n",
    "        y_pred_prod[test_fold == i] = test_pred_prod\n",
    "        print \"fold#%d done in %0.3fs, score: %0.3f.\" % (i, time()-t0, accuracy_score(y_test, test_pred))\n",
    "    t0 = time()\n",
    "    print \"Retraining classifier with whole train set...\"\n",
    "    clf_train(clf, X, y)\n",
    "    print \"Done in %0.3fs.\" % (time() - t0)\n",
    "    return y_pred, y_pred_sum, y_pred_prod\n",
    "\n",
    "def clf_train(clf, files, file_labels):\n",
    "    X_train, y_train= parse_audio_files(files, file_labels)\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "def predict_maj(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_pred = np.append(y_pred, mode(y_predicts).mode[0])\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_sum(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = np.sum(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_prod(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = np.prod(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def clf_predict(clf, X_test):\n",
    "    y_pred = np.empty(0)\n",
    "    y_pred_sum = np.empty(0)\n",
    "    y_pred_prod = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_predict_probas = clf.predict_proba(x_mfccs)\n",
    "        y_pred = np.append(y_pred, mode(y_predicts).mode[0])\n",
    "        y_pred_sum = np.append(y_pred_sum, np.argmax(np.sum(y_predict_probas, axis=0)))\n",
    "        y_pred_prod = np.append(y_pred_prod, np.argmax(np.prod(y_predict_probas, axis=0)))\n",
    "    return np.array(y_pred, dtype=np.int), np.array(y_pred_sum, dtype=np.int), np.array(y_pred_prod, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Fold #1: 290 files from 45 sources\n",
      "Fold #2: 292 files from 43 sources\n",
      "Fold #3: 290 files from 45 sources\n",
      "15 scenes: ['beach' 'bus' 'cafe/restaurant' 'car' 'city_center' 'forest_path'\n",
      " 'grocery_store' 'home' 'library' 'metro_station' 'office' 'park'\n",
      " 'residential_area' 'train' 'tram']\n",
      "Training set size: 872\n",
      "Test set size: 298\n",
      "Done in 0.016s.\n"
     ]
    }
   ],
   "source": [
    "# Read data and preprocessing\n",
    "print \"Loading files...\"\n",
    "t0 = time()\n",
    "\n",
    "# Define FILEROOT according to the platform\n",
    "# My personal computer\n",
    "if platform == \"darwin\":\n",
    "    FILEROOT = './'\n",
    "# The machines of Telecom\n",
    "else:\n",
    "    FILEROOT = '/tsi/plato/sons/sd207/'\n",
    "\n",
    "# Load the cross validation folds\n",
    "N_FOLDS = 3\n",
    "train_files, train_scenes, test_fold = np.empty(0, dtype=str), np.empty(0), np.empty(0)\n",
    "for i in range(N_FOLDS):\n",
    "    files = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[0].values\n",
    "    scenes = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[1].values\n",
    "    print \"Fold #%d: %d files from %d sources\" % (i+1, len(files), len(np.unique([f.split('_')[0] for f in files])))\n",
    "    train_files = np.append(train_files, files)\n",
    "    train_scenes = np.append(train_scenes, scenes)\n",
    "    test_fold = np.append(test_fold, i*np.ones_like(scenes))\n",
    "\n",
    "scenes = np.unique(train_scenes)\n",
    "n_scenes = len(scenes)\n",
    "labels = pd.factorize(scenes)[0]\n",
    "n_labels = len(labels)\n",
    "train_labels = pd.factorize(train_scenes)[0]\n",
    "test_files = pd.read_csv('test_files.txt', header=None)[0].values\n",
    "\n",
    "print \"%d scenes:\" % n_scenes, scenes\n",
    "print \"Training set size: %d\" % len(train_files)\n",
    "print \"Test set size: %d\" % len(test_files)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing cross validation...\n",
      "Launching fold #1/3\n",
      "train set size: 582, test set size: 290\n",
      "fold#0 done in 62.610s, score: 0.566.\n",
      "Launching fold #2/3\n",
      "train set size: 580, test set size: 292\n",
      "fold#1 done in 37.846s, score: 0.651.\n",
      "Launching fold #3/3\n",
      "train set size: 582, test set size: 290\n",
      "fold#2 done in 38.880s, score: 0.624.\n",
      "Retraining classifier with whole train set...\n",
      "Done in 68.253s.\n",
      "Done in 207.592s.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "print \"Doing cross validation...\"\n",
    "t0 = time()\n",
    "\n",
    "np.random.seed(42)\n",
    "n_mfcc = 40\n",
    "n_features = 40\n",
    "file_features = {}\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(40), alpha=0.1)\n",
    "y_pred, y_pred_sum, y_pred_prod = cross_validation(clf, train_files, train_labels, test_fold)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on validation test (vote by majority): 0.613532\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.79      0.51      0.62        59\n",
      "             bus       0.61      0.68      0.64        59\n",
      " cafe/restaurant       0.86      0.54      0.67        57\n",
      "             car       0.78      0.68      0.73        59\n",
      "     city_center       0.83      0.90      0.86        59\n",
      "     forest_path       0.74      0.83      0.78        60\n",
      "   grocery_store       0.70      0.80      0.75        59\n",
      "            home       0.62      0.34      0.44        58\n",
      "         library       0.63      0.84      0.72        58\n",
      "   metro_station       0.46      0.57      0.51        56\n",
      "          office       0.67      0.68      0.68        60\n",
      "            park       0.28      0.21      0.24        58\n",
      "residential_area       0.36      0.53      0.43        59\n",
      "           train       0.57      0.22      0.32        55\n",
      "            tram       0.51      0.84      0.63        56\n",
      "\n",
      "     avg / total       0.63      0.61      0.60       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[30  0  0  1  0  0  0  3  5  0  1  2 10  0  7]\n",
      " [ 0 40  0  2  0  0  0  0  0  0  0  0  0  9  8]\n",
      " [ 0  0 31  0  0  1 16  0  1  8  0  0  0  0  0]\n",
      " [ 0  6  0 40  0  0  0  0  0  0  0  0  0  0 13]\n",
      " [ 0  0  0  0 53  0  2  0  0  1  0  0  3  0  0]\n",
      " [ 0  0  3  0  0 50  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0 47  0  5  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0 20  5  3 16  2  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 49  8  0  0  0  0  1]\n",
      " [ 0  0  2  0  0  0  2  1  3 32  2 10  0  0  4]\n",
      " [ 0  0  0  0  0  2  0  8  0  9 41  0  0  0  0]\n",
      " [ 7  0  0  0  2  0  0  0  4  1  1 12 31  0  0]\n",
      " [ 0  0  0  0  6  6  0  0  0  1  0 15 31  0  0]\n",
      " [ 1 17  0  3  3  0  0  0  6  0  0  0  0 12 13]\n",
      " [ 0  3  0  5  0  0  0  0  0  0  0  1  0  0 47]]\n",
      "Score on validation test (vote by proba sum): 0.621560\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.79      0.53      0.63        59\n",
      "             bus       0.60      0.68      0.63        59\n",
      " cafe/restaurant       0.86      0.56      0.68        57\n",
      "             car       0.80      0.68      0.73        59\n",
      "     city_center       0.82      0.90      0.85        59\n",
      "     forest_path       0.75      0.85      0.80        60\n",
      "   grocery_store       0.71      0.80      0.75        59\n",
      "            home       0.65      0.38      0.48        58\n",
      "         library       0.65      0.84      0.74        58\n",
      "   metro_station       0.48      0.59      0.53        56\n",
      "          office       0.67      0.68      0.68        60\n",
      "            park       0.30      0.21      0.24        58\n",
      "residential_area       0.39      0.58      0.46        59\n",
      "           train       0.55      0.20      0.29        55\n",
      "            tram       0.49      0.82      0.62        56\n",
      "\n",
      "     avg / total       0.64      0.62      0.61       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[31  0  0  1  0  0  0  3  4  0  1  2 10  0  7]\n",
      " [ 0 40  0  1  0  0  0  0  0  0  0  0  0  9  9]\n",
      " [ 0  0 32  0  0  1 16  0  1  7  0  0  0  0  0]\n",
      " [ 0  6  0 40  0  0  0  0  0  0  0  0  0  0 13]\n",
      " [ 0  0  0  0 53  0  1  0  0  1  0  0  4  0  0]\n",
      " [ 0  0  3  0  0 51  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0 47  0  5  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  8  0 22  4  3 16  2  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 49  8  0  0  0  0  1]\n",
      " [ 0  0  2  0  0  0  2  1  2 33  2 10  0  0  4]\n",
      " [ 0  0  0  0  0  2  0  8  0  9 41  0  0  0  0]\n",
      " [ 7  0  0  0  2  0  0  0  4  1  1 12 31  0  0]\n",
      " [ 0  0  0  0  6  6  0  0  0  1  0 12 34  0  0]\n",
      " [ 1 17  0  3  4  0  0  0  6  0  0  0  0 11 13]\n",
      " [ 0  4  0  5  0  0  0  0  0  0  0  1  0  0 46]]\n",
      "Score on validation test (vote by proba product): 0.606651\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.49      0.54      0.52        59\n",
      "             bus       0.63      0.63      0.63        59\n",
      " cafe/restaurant       0.77      0.40      0.53        57\n",
      "             car       0.79      0.56      0.65        59\n",
      "     city_center       0.90      0.90      0.90        59\n",
      "     forest_path       0.82      0.88      0.85        60\n",
      "   grocery_store       0.71      0.80      0.75        59\n",
      "            home       0.59      0.41      0.48        58\n",
      "         library       0.71      0.86      0.78        58\n",
      "   metro_station       0.40      0.52      0.45        56\n",
      "          office       0.72      0.63      0.67        60\n",
      "            park       0.34      0.21      0.26        58\n",
      "residential_area       0.42      0.63      0.50        59\n",
      "           train       0.52      0.22      0.31        55\n",
      "            tram       0.48      0.88      0.62        56\n",
      "\n",
      "     avg / total       0.62      0.61      0.60       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[32  0  0  0  0  0  0  3  4  0  0  2 10  0  8]\n",
      " [ 0 37  1  2  0  0  0  0  0  0  0  0  0  9 10]\n",
      " [ 3  0 23  0  0  0 16  1  0 12  0  0  2  0  0]\n",
      " [ 3  7  0 33  0  0  0  0  0  0  0  0  0  1 15]\n",
      " [ 0  0  0  0 53  0  1  0  0  1  0  0  4  0  0]\n",
      " [ 0  0  3  0  0 53  0  0  0  0  0  1  3  0  0]\n",
      " [ 0  0  0  0  0  0 47  0  5  6  0  1  0  0  0]\n",
      " [ 4  0  0  0  0  5  0 24  3  6 13  1  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 50  6  0  0  0  0  2]\n",
      " [ 4  0  3  0  0  0  2  2  2 29  2  8  0  0  4]\n",
      " [ 0  0  0  0  0  2  0 11  0  9 38  0  0  0  0]\n",
      " [ 8  0  0  0  1  0  0  0  4  1  0 12 31  1  0]\n",
      " [ 3  0  0  0  2  5  0  0  0  2  0 10 37  0  0]\n",
      " [ 5 15  0  3  3  0  0  0  2  0  0  0  0 12 15]\n",
      " [ 3  0  0  4  0  0  0  0  0  0  0  0  0  0 49]]\n",
      "Done in 0.013s.\n"
     ]
    }
   ],
   "source": [
    "# Predicting on validation set...\n",
    "t0 = time()\n",
    "print \"Score on validation test (vote by majority): %f\" % accuracy_score(train_labels, y_pred)\n",
    "print classification_report(train_labels, y_pred, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred)\n",
    "\n",
    "print \"Score on validation test (vote by proba sum): %f\" % accuracy_score(train_labels, y_pred_sum )\n",
    "print classification_report(train_labels, y_pred_sum, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_sum)\n",
    "\n",
    "print \"Score on validation test (vote by proba product): %f\" % accuracy_score(train_labels, y_pred_prod)\n",
    "print classification_report(train_labels, y_pred_prod, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_prod)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  8  3  7 14  5  7  0  3  7 10 12  6 14 12  5 12  8  0  8  8  6  7  7 11\n",
      " 12  2  7  2  3 14 14 10 12  8  5  3  3  0  4  7  0  0  7  6 11  0  6  3  8\n",
      "  0  6 11 12 12 10 11  6  5 11  7  4  0 11  2 14  9  7  3  4  0 14 11  7 14\n",
      "  2  1  9  1 10 12  0  4  2  4  3 14 13  1  9 12 14 10  6  1  1  7 10  5  0\n",
      "  1  1  6  6 10  0  7  5  5 14  4 12 14  2  3  0  0  9  9 10 13  3  4  9  1\n",
      "  1 14 12  4  5 14  7  1 10  2  2 11 12 12 14 12  5 12  9  3  3  9  7  1 11\n",
      "  7  3 14 13  9 13 12  0 14  6  7  4 11  9 14  1  9  5 10  7 14 13  0 12 11\n",
      "  0  8  7 12  5  5  6  4  2  5  2  4 14  7  9 14  7  4 11  9  3  9  2  8  0\n",
      "  1 10 14  3 11  0  4 14  0  4  9  6  6 10 11  2  6 14  2 14  2  7  3  2 11\n",
      " 12 13  1  2  2  9 11 12 10  4 10 11  5  4  7  8  7  3 12  9  5  4  7  0  1\n",
      "  4  2 14  6 10  5 14  5  1  8 10  7 12  7 11 12  5 12 14  4 10  3  6 14  4\n",
      " 11  2 14  8  7  9  5  3 14  2  6 10 14  4  0  9  7  0  9  8  9  5 12]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred, y_test_pred_sum, y_test_pred_prod = clf_predict(clf, test_files)\n",
    "np.savetxt('y_test_pred_mfcc_mlp.txt', y_test_pred, fmt='%d')\n",
    "np.savetxt('y_test_pred_mfcc_mlp_sum.txt', y_test_pred_sum, fmt='%d')\n",
    "np.savetxt('y_test_pred_mfcc_mlp_prod.txt', y_test_pred_prod, fmt='%d')\n",
    "print y_test_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
