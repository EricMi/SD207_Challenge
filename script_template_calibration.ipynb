{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge SD207 - 2017\n",
    "*<p>Author: Pengfei MI, Rui SONG</p>*\n",
    "*<p>Date: 06/06/2017</p>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "from time import time\n",
    "\n",
    "# Librosa related: audio feature extraction\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Sklearn related: data preprocessing and classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some usefull functions\n",
    "def load_sound_file(file_name):\n",
    "    X, sr = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    return X\n",
    "\n",
    "def extract_feature(file_name, feature_type): # Late fusion\n",
    "    X, sample_rate = librosa.load(os.path.join(FILEROOT, file_name), sr=None)\n",
    "    if feature_type == 'mfcc':\n",
    "        return librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "    elif feature_type == 'mfcc_0':\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)[1:].T\n",
    "        return mfcc\n",
    "    elif feature_type == \"d_mfcc\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        return librosa.feature.delta(mfcc, width=width, order=1, trim=True)\n",
    "    elif feature_type == \"dd_mfcc\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        return librosa.feature.delta(mfcc, width=width, order=2, trim=True)\n",
    "    elif feature_type == \"mfcc_d\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        d_mfcc = librosa.feature.delta(mfcc, width=width, order=1, trim=True)\n",
    "        return np.c_[mfcc, d_mfcc]\n",
    "    elif feature_type == \"mfcc_dd\":\n",
    "        mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc).T\n",
    "        d_mfcc = librosa.feature.delta(mfcc, width=width, order=1, trim=True)\n",
    "        dd_mfcc = librosa.feature.delta(mfcc, width=width, order=2, trim=True)\n",
    "        return np.c_[mfcc, d_mfcc, dd_mfcc]\n",
    "    \n",
    "def parse_audio_files(file_names, file_labels, feature_type):\n",
    "    # Try to detect the feature dimension\n",
    "    n_features = extract_feature(file_names[0], feature_type).shape[1]\n",
    "    features, labels = np.empty((0, n_features)), np.empty(0)\n",
    "    for fn, fl in zip(file_names, file_labels):\n",
    "        ff = extract_feature(fn, feature_type)\n",
    "        features = np.vstack([features, ff])\n",
    "        labels = np.append(labels, fl*np.ones(ff.shape[0]))\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "def cross_validation(clf, X, y, test_fold, feature_type=\"mfcc\"):\n",
    "    y_pred, y_pred_sum, y_pred_prod = np.empty_like(y), np.empty_like(y), np.empty_like(y)\n",
    "    n_folds = len(np.unique(test_fold))\n",
    "    for i in range(n_folds):\n",
    "        t0 = time()\n",
    "        new_clf = clone(clf, safe=True)\n",
    "        X_train = X[test_fold != i]\n",
    "        X_test = X[test_fold == i]\n",
    "        y_train = y[test_fold != i]\n",
    "        y_test = y[test_fold == i]\n",
    "        print \"Launching fold #%d/%d, train set size: %d, test set size: %d\" % (i+1, n_folds, len(X_train), len(X_test))\n",
    "        clf_train(new_clf, X_train, y_train, feature_type)\n",
    "        test_pred, test_pred_sum, test_pred_prod = clf_predict(new_clf, X_test, feature_type)\n",
    "        y_pred[test_fold == i] = test_pred\n",
    "        y_pred_sum[test_fold == i] = test_pred_sum\n",
    "        y_pred_prod[test_fold == i] = test_pred_prod\n",
    "        print \"fold#%d done in %0.3fs, score: %0.3f.\" % (i+1, time()-t0, accuracy_score(y_test, test_pred))\n",
    "    t0 = time()\n",
    "    print \"Retraining classifier with whole train set...\"\n",
    "    clf_train(clf, X, y, feature_type)\n",
    "    print \"Done in %0.3fs.\" % (time() - t0)\n",
    "    return y_pred, y_pred_sum, y_pred_prod\n",
    "\n",
    "def clf_train(clf, files, file_labels, feature_type):\n",
    "    X_train, y_train= parse_audio_files(files, file_labels, feature_type)\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "def predict_maj(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_pred = np.append(y_pred, sp.stats.mode(y_predicts).mode[0])\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_sum(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = np.sum(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def predict_prod(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = np.prod(clf.predict_proba(x_mfccs), axis=0)\n",
    "        y_pred = np.append(y_pred, np.argmax(y_predicts))\n",
    "    return np.array(y_pred, dtype = np.int)\n",
    "\n",
    "def clf_predict(clf, X_test, feature_type):\n",
    "    y_pred = np.empty(0)\n",
    "    y_pred_sum = np.empty(0)\n",
    "    y_pred_prod = np.empty(0)\n",
    "    for x in X_test:\n",
    "        x_mfccs = extract_feature(x, feature_type)\n",
    "        y_predicts = clf.predict(x_mfccs)\n",
    "        y_predict_probas = clf.predict_proba(x_mfccs)\n",
    "        y_pred = np.append(y_pred, sp.stats.mode(y_predicts).mode[0])\n",
    "        y_pred_sum = np.append(y_pred_sum, np.argmax(np.sum(y_predict_probas, axis=0)))\n",
    "        y_pred_prod = np.append(y_pred_prod, np.argmax(np.prod(y_predict_probas, axis=0)))\n",
    "    return np.array(y_pred, dtype=np.int), np.array(y_pred_sum, dtype=np.int), np.array(y_pred_prod, dtype=np.int)\n",
    "\n",
    "def predict_calibration(clfs, feature_types, pcss, X_test):\n",
    "    y_pred, y_pred_sum = np.empty(0), np.empty(0)\n",
    "    for x in X_test:\n",
    "        max_pcs, max_idx = -1,-1\n",
    "        proba_sum = np.empty(n_labels)\n",
    "        for (i, clf) in enumerate(clfs):\n",
    "            x_feature = extract_feature(x, feature_types[i])\n",
    "            x_pred_proba = np.mean(clf.predict_proba(x_feature), axis=0)\n",
    "            proba_sum += x_pred_proba*pcss[i]\n",
    "            res = np.argmax(x_pred_proba)\n",
    "            if pcss[i][res] > max_pcs:\n",
    "                max_pcs = pcss[i][res]\n",
    "                max_idx = res\n",
    "        y_pred = np.append(y_pred, max_idx)\n",
    "        y_pred_sum = np.append(y_pred_sum, np.argmax(proba_sum))\n",
    "    return np.array(y_pred, dtype=np.int), np.array(y_pred_sum, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Fold #1: 290 files from 45 sources\n",
      "Fold #2: 292 files from 43 sources\n",
      "Fold #3: 290 files from 45 sources\n",
      "15 scenes: ['beach' 'bus' 'cafe/restaurant' 'car' 'city_center' 'forest_path'\n",
      " 'grocery_store' 'home' 'library' 'metro_station' 'office' 'park'\n",
      " 'residential_area' 'train' 'tram']\n",
      "Training set size: 872\n",
      "Test set size: 298\n",
      "Done in 0.019s.\n"
     ]
    }
   ],
   "source": [
    "# Read data and preprocessing\n",
    "print \"Loading files...\"\n",
    "t0 = time()\n",
    "\n",
    "# Define FILEROOT according to the platform\n",
    "# My personal computer\n",
    "if sys.platform == \"darwin\":\n",
    "    FILEROOT = './'\n",
    "# Node of Telecom\n",
    "elif platform.node()[:4] == 'lame':\n",
    "    FILEROOT = '/tmp/'\n",
    "# The machines of Telecom\n",
    "else:\n",
    "    FILEROOT = '/tsi/plato/sons/sd207/'\n",
    "\n",
    "# Load the cross validation folds\n",
    "N_FOLDS = 3\n",
    "train_files, train_scenes, test_fold = np.empty(0, dtype=str), np.empty(0), np.empty(0)\n",
    "for i in range(N_FOLDS):\n",
    "    files = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[0].values\n",
    "    scenes = pd.read_csv('train%s.txt' % str(i), sep='\\s+', header=None)[1].values\n",
    "    print \"Fold #%d: %d files from %d sources\" % (i+1, len(files), len(np.unique([f.split('_')[0] for f in files])))\n",
    "    train_files = np.append(train_files, files)\n",
    "    train_scenes = np.append(train_scenes, scenes)\n",
    "    test_fold = np.append(test_fold, i*np.ones_like(scenes))\n",
    "\n",
    "scenes = np.unique(train_scenes)\n",
    "n_scenes = len(scenes)\n",
    "labels = pd.factorize(scenes, sort=True)[0]\n",
    "n_labels = len(labels)\n",
    "train_labels = pd.factorize(train_scenes, sort=True)[0]\n",
    "test_files = pd.read_csv('test_files.txt', header=None)[0].values\n",
    "test_labels = pd.read_csv('meta.txt', header=None)[0].values\n",
    "\n",
    "print \"%d scenes:\" % n_scenes, scenes\n",
    "print \"Training set size: %d\" % len(train_files)\n",
    "print \"Test set size: %d\" % len(test_files)\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing cross validation...\n",
      "Launching fold #1/3, train set size: 582, test set size: 290\n",
      "fold#1 done in 70.095s, score: 0.614.\n",
      "Launching fold #2/3, train set size: 580, test set size: 292\n",
      "fold#2 done in 79.809s, score: 0.668.\n",
      "Launching fold #3/3, train set size: 582, test set size: 290\n",
      "fold#3 done in 54.931s, score: 0.562.\n",
      "Retraining classifier with whole train set...\n",
      "Done in 110.342s.\n",
      "Launching fold #1/3, train set size: 582, test set size: 290\n",
      "fold#1 done in 75.532s, score: 0.628.\n",
      "Launching fold #2/3, train set size: 580, test set size: 292\n",
      "fold#2 done in 78.747s, score: 0.630.\n",
      "Launching fold #3/3, train set size: 582, test set size: 290\n",
      "fold#3 done in 68.202s, score: 0.559.\n",
      "Retraining classifier with whole train set...\n",
      "Done in 127.673s.\n",
      "Done in 665.335s.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "print \"Doing cross validation...\"\n",
    "t0 = time()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#feature_type = \"mfcc\"\n",
    "n_mfcc = 40\n",
    "n_fft = 4096\n",
    "hop_length = 2048\n",
    "width = 7\n",
    "\n",
    "clf1 = MLPClassifier(hidden_layer_sizes=(40), alpha=0.01)\n",
    "y_pred1, y_pred_sum1, y_pred_prod1 = cross_validation(clf1, train_files, train_labels, test_fold, \"mfcc\")\n",
    "\n",
    "\n",
    "n_mfcc = 40\n",
    "n_fft = 2048\n",
    "hop_length = 1024\n",
    "width = 7\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(40), alpha=0.01)\n",
    "y_pred2, y_pred_sum2, y_pred_prod2 = cross_validation(clf2, train_files, train_labels, test_fold, \"mfcc\")\n",
    "print \"Done in %0.3fs.\" % (time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Prediction result via mfcc features:\n",
      "Score on validation test (vote by majority): 0.614679\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.72      0.49      0.59        59\n",
      "             bus       0.59      0.58      0.58        59\n",
      " cafe/restaurant       0.74      0.35      0.48        57\n",
      "             car       0.90      0.64      0.75        59\n",
      "     city_center       0.88      0.88      0.88        59\n",
      "     forest_path       0.78      0.95      0.86        60\n",
      "   grocery_store       0.76      0.69      0.73        59\n",
      "            home       0.56      0.47      0.51        58\n",
      "         library       0.62      0.83      0.71        58\n",
      "   metro_station       0.44      0.59      0.50        56\n",
      "          office       0.72      0.73      0.73        60\n",
      "            park       0.49      0.29      0.37        58\n",
      "residential_area       0.38      0.56      0.45        59\n",
      "           train       0.70      0.25      0.37        55\n",
      "            tram       0.43      0.88      0.57        56\n",
      "\n",
      "     avg / total       0.65      0.61      0.61       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[29  0  0  1  0  0  0  2  3  0  4  1 12  0  7]\n",
      " [ 0 34  1  1  0  0  0  0  0  0  0  0  0  6 17]\n",
      " [ 0  0 20  0  0  0 12  3  7 12  0  0  0  0  3]\n",
      " [ 2  4  0 38  0  0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  1  0 52  0  1  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0 57  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  1  0  0  0 41  0  5 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0 27  6  3 10  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 48  8  0  0  0  0  2]\n",
      " [ 0  0  4  0  0  0  0  5  2 33  2  7  0  0  3]\n",
      " [ 0  0  0  0  0  0  0 11  0  5 44  0  0  0  0]\n",
      " [ 5  0  0  0  0  0  0  0  4  0  1 17 31  0  0]\n",
      " [ 4  0  0  0  4  7  0  0  0  2  0  9 33  0  0]\n",
      " [ 0 17  0  0  3  0  0  0  2  0  0  0  0 14 19]\n",
      " [ 0  3  0  2  0  0  0  0  1  0  0  1  0  0 49]]\n",
      "Score on validation test (vote by proba sum): 0.620413\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.75      0.51      0.61        59\n",
      "             bus       0.59      0.58      0.58        59\n",
      " cafe/restaurant       0.82      0.32      0.46        57\n",
      "             car       0.93      0.64      0.76        59\n",
      "     city_center       0.88      0.86      0.87        59\n",
      "     forest_path       0.78      0.95      0.86        60\n",
      "   grocery_store       0.76      0.71      0.74        59\n",
      "            home       0.55      0.47      0.50        58\n",
      "         library       0.63      0.83      0.72        58\n",
      "   metro_station       0.45      0.61      0.52        56\n",
      "          office       0.71      0.77      0.74        60\n",
      "            park       0.49      0.29      0.37        58\n",
      "residential_area       0.39      0.58      0.46        59\n",
      "           train       0.71      0.27      0.39        55\n",
      "            tram       0.43      0.89      0.58        56\n",
      "\n",
      "     avg / total       0.66      0.62      0.61       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[30  0  0  1  0  0  0  2  2  0  5  1 11  0  7]\n",
      " [ 0 34  1  1  0  0  0  0  0  0  0  0  0  6 17]\n",
      " [ 0  0 18  0  0  0 12  3  7 13  0  0  1  0  3]\n",
      " [ 1  5  0 38  0  0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0 51  0  1  0  0  1  0  0  6  0  0]\n",
      " [ 0  0  0  0  0 57  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0 42  0  5 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0 27  5  3 11  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 48  8  0  0  0  0  2]\n",
      " [ 0  0  3  0  0  0  0  6  2 34  2  6  0  0  3]\n",
      " [ 0  0  0  0  0  0  0 11  0  3 46  0  0  0  0]\n",
      " [ 5  0  0  0  0  1  0  0  4  0  1 17 30  0  0]\n",
      " [ 4  0  0  0  4  6  0  0  0  1  0 10 34  0  0]\n",
      " [ 0 16  0  0  3  0  0  0  2  0  0  0  0 15 19]\n",
      " [ 0  3  0  1  0  0  0  0  1  0  0  1  0  0 50]]\n",
      "Score on validation test (vote by proba product): 0.614679\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.54      0.54      0.54        59\n",
      "             bus       0.65      0.56      0.60        59\n",
      " cafe/restaurant       0.65      0.19      0.30        57\n",
      "             car       1.00      0.58      0.73        59\n",
      "     city_center       0.93      0.85      0.88        59\n",
      "     forest_path       0.80      0.95      0.87        60\n",
      "   grocery_store       0.77      0.69      0.73        59\n",
      "            home       0.57      0.50      0.53        58\n",
      "         library       0.64      0.84      0.73        58\n",
      "   metro_station       0.44      0.62      0.51        56\n",
      "          office       0.78      0.78      0.78        60\n",
      "            park       0.54      0.26      0.35        58\n",
      "residential_area       0.40      0.63      0.49        59\n",
      "           train       0.71      0.27      0.39        55\n",
      "            tram       0.41      0.91      0.57        56\n",
      "\n",
      "     avg / total       0.66      0.61      0.60       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[32  0  0  0  0  0  0  2  3  0  2  1 12  0  7]\n",
      " [ 0 33  1  0  0  0  0  0  0  0  0  0  0  6 19]\n",
      " [ 2  0 11  0  0  0 11  3  7 18  0  0  1  0  4]\n",
      " [ 3  4  0 34  0  0  0  0  0  0  0  0  0  0 18]\n",
      " [ 0  0  0  0 50  0  1  0  0  1  0  0  7  0  0]\n",
      " [ 0  0  0  0  0 57  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0 41  0  5 13  0  0  0  0  0]\n",
      " [ 4  0  0  0  0  8  0 29  5  2  8  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 49  7  0  0  0  0  2]\n",
      " [ 2  0  4  0  0  0  0  5  1 35  2  4  0  0  3]\n",
      " [ 0  0  0  0  0  0  0 11  0  2 47  0  0  0  0]\n",
      " [ 5  0  0  0  0  1  0  1  4  0  1 15 31  0  0]\n",
      " [ 5  0  0  0  2  5  0  0  0  2  0  8 37  0  0]\n",
      " [ 3 14  0  0  2  0  0  0  1  0  0  0  0 15 20]\n",
      " [ 3  0  1  0  0  0  0  0  1  0  0  0  0  0 51]]\n",
      "Precision1:  [ 0.725       0.5862069   0.74074074  0.9047619   0.88135593  0.78082192\n",
      "  0.75925926  0.5625      0.61538462  0.44        0.72131148  0.48571429\n",
      "  0.37931034  0.7         0.42608696]\n",
      "------------------------------------------------------------\n",
      "Prediction result via delta mfcc features:\n",
      "Score on validation test (vote by majority): 0.605505\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.68      0.69      0.69        59\n",
      "             bus       0.46      0.39      0.42        59\n",
      " cafe/restaurant       0.75      0.37      0.49        57\n",
      "             car       0.81      0.81      0.81        59\n",
      "     city_center       0.73      0.80      0.76        59\n",
      "     forest_path       0.79      0.92      0.85        60\n",
      "   grocery_store       0.74      0.78      0.76        59\n",
      "            home       1.00      0.55      0.71        58\n",
      "         library       0.53      0.64      0.58        58\n",
      "   metro_station       0.46      0.64      0.54        56\n",
      "          office       0.74      1.00      0.85        60\n",
      "            park       0.29      0.19      0.23        58\n",
      "residential_area       0.28      0.31      0.29        59\n",
      "           train       0.36      0.16      0.22        55\n",
      "            tram       0.49      0.79      0.60        56\n",
      "\n",
      "     avg / total       0.61      0.61      0.59       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[41  0  0  2  0  0  0  0  1  0  0  2  8  0  5]\n",
      " [ 0 23  2  0  0  0  1  0  0  2  0  0  0 12 19]\n",
      " [ 0  0 21  0  0  7 13  0  1 14  0  1  0  0  0]\n",
      " [ 0  1  0 48  0  0  0  0  0  0  0  0  0  0 10]\n",
      " [ 0  1  0  0 47  0  1  0  0  1  0  0  9  0  0]\n",
      " [ 0  0  0  0  0 55  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0 46  0  5  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 32 10  3 12  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0 37 11  5  0  0  2  2]\n",
      " [ 0  0  4  0  0  0  1  0  4 36  2  8  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 60  0  0  0  0]\n",
      " [11  1  0  0  1  0  0  0  7  0  2 11 25  0  0]\n",
      " [ 5  0  0  0 10  7  0  0  2  2  0 15 18  0  0]\n",
      " [ 3 19  0  4  6  0  0  0  3  1  0  0  0  9 10]\n",
      " [ 0  5  0  5  0  0  0  0  0  0  0  1  0  1 44]]\n",
      "Score on validation test (vote by proba sum): 0.604358\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.69      0.69      0.69        59\n",
      "             bus       0.45      0.39      0.42        59\n",
      " cafe/restaurant       0.79      0.39      0.52        57\n",
      "             car       0.81      0.78      0.79        59\n",
      "     city_center       0.72      0.80      0.76        59\n",
      "     forest_path       0.80      0.92      0.85        60\n",
      "   grocery_store       0.74      0.78      0.76        59\n",
      "            home       0.97      0.55      0.70        58\n",
      "         library       0.52      0.59      0.55        58\n",
      "   metro_station       0.48      0.70      0.57        56\n",
      "          office       0.72      1.00      0.84        60\n",
      "            park       0.30      0.19      0.23        58\n",
      "residential_area       0.27      0.29      0.28        59\n",
      "           train       0.38      0.18      0.25        55\n",
      "            tram       0.48      0.79      0.60        56\n",
      "\n",
      "     avg / total       0.61      0.60      0.59       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[41  0  0  2  0  0  0  0  1  0  0  1  8  0  6]\n",
      " [ 0 23  2  0  0  0  1  0  0  2  0  0  0 12 19]\n",
      " [ 0  0 22  0  0  6 13  0  1 14  0  1  0  0  0]\n",
      " [ 0  3  0 46  0  0  0  0  0  0  0  0  0  0 10]\n",
      " [ 0  1  0  0 47  0  1  0  0  1  0  0  9  0  0]\n",
      " [ 0  0  0  0  0 55  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0 46  0  5  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 32 10  3 12  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0 34 12  7  0  0  2  2]\n",
      " [ 0  0  3  0  0  0  1  0  2 39  2  8  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 60  0  0  0  0]\n",
      " [10  1  0  0  1  0  0  1  7  0  2 11 25  0  0]\n",
      " [ 5  0  0  0 11  7  0  0  3  1  0 15 17  0  0]\n",
      " [ 3 18  0  4  6  0  0  0  3  1  0  0  0 10 10]\n",
      " [ 0  5  0  5  0  0  0  0  0  0  0  1  0  1 44]]\n",
      "Score on validation test (vote by proba product): 0.603211\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           beach       0.64      0.66      0.65        59\n",
      "             bus       0.45      0.37      0.41        59\n",
      " cafe/restaurant       0.72      0.32      0.44        57\n",
      "             car       0.82      0.76      0.79        59\n",
      "     city_center       0.77      0.80      0.78        59\n",
      "     forest_path       0.86      0.92      0.89        60\n",
      "   grocery_store       0.75      0.78      0.77        59\n",
      "            home       0.89      0.59      0.71        58\n",
      "         library       0.49      0.64      0.56        58\n",
      "   metro_station       0.43      0.70      0.53        56\n",
      "          office       0.82      1.00      0.90        60\n",
      "            park       0.28      0.16      0.20        58\n",
      "residential_area       0.30      0.36      0.33        59\n",
      "           train       0.34      0.18      0.24        55\n",
      "            tram       0.49      0.79      0.60        56\n",
      "\n",
      "     avg / total       0.61      0.60      0.59       872\n",
      "\n",
      "Confusion matrix:\n",
      "[[39  0  0  1  0  0  0  1  1  0  0  3  9  0  5]\n",
      " [ 0 22  2  0  0  0  1  0  0  0  0  0  0 15 19]\n",
      " [ 1  0 18  0  0  2 12  0  3 21  0  0  0  0  0]\n",
      " [ 0  3  0 45  0  0  0  0  0  0  0  0  0  1 10]\n",
      " [ 0  1  0  0 47  0  1  0  0  1  0  0  9  0  0]\n",
      " [ 0  0  0  0  0 55  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0 46  0  5  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 34 13  5  5  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0 37 12  4  0  0  2  2]\n",
      " [ 0  0  4  0  0  0  1  2  4 39  2  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 60  0  0  0  0]\n",
      " [11  1  0  0  1  0  0  1  7  1  2  9 25  0  0]\n",
      " [ 5  0  0  0  7  6  0  0  3  1  0 16 21  0  0]\n",
      " [ 5 16  0  5  6  0  0  0  2  1  0  0  0 10 10]\n",
      " [ 0  6  0  4  0  0  0  0  0  1  0  0  0  1 44]]\n",
      "Precision2:  [ 0.68333333  0.46        0.75        0.81355932  0.734375    0.78571429\n",
      "  0.74193548  1.          0.52857143  0.46153846  0.74074074  0.28947368\n",
      "  0.27692308  0.36        0.48888889]\n"
     ]
    }
   ],
   "source": [
    "# Print cross validation results\n",
    "t0 = time()\n",
    "print '-'*60\n",
    "print \"Prediction result via mfcc features:\"\n",
    "print \"Score on validation test (vote by majority): %f\" % accuracy_score(train_labels, y_pred1)\n",
    "print classification_report(train_labels, y_pred1, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred1)\n",
    "\n",
    "print \"Score on validation test (vote by proba sum): %f\" % accuracy_score(train_labels, y_pred_sum1)\n",
    "print classification_report(train_labels, y_pred_sum1, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_sum1)\n",
    "\n",
    "print \"Score on validation test (vote by proba product): %f\" % accuracy_score(train_labels, y_pred_prod1)\n",
    "print classification_report(train_labels, y_pred_prod1, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_prod1)\n",
    "precision1 = precision_score(train_labels, y_pred1, average=None)\n",
    "print \"Precision1: \", precision1\n",
    "\n",
    "print '-'*60\n",
    "print \"Prediction result via delta mfcc features:\"\n",
    "print \"Score on validation test (vote by majority): %f\" % accuracy_score(train_labels, y_pred2)\n",
    "print classification_report(train_labels, y_pred2, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred2)\n",
    "\n",
    "print \"Score on validation test (vote by proba sum): %f\" % accuracy_score(train_labels, y_pred_sum2)\n",
    "print classification_report(train_labels, y_pred_sum2, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_sum2)\n",
    "\n",
    "print \"Score on validation test (vote by proba product): %f\" % accuracy_score(train_labels, y_pred_prod2)\n",
    "print classification_report(train_labels, y_pred_prod2, target_names=scenes)\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(train_labels, y_pred_prod2)\n",
    "precision2 = precision_score(train_labels, y_pred2, average=None)\n",
    "print \"Precision2: \", precision2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score by maj: 0.768456\n",
      "Score by sum: 0.644295\n"
     ]
    }
   ],
   "source": [
    "y_test_pred, y_test_pred_sum= predict_calibration([clf1, clf2], [\"mfcc\", \"d_mfcc\"], [precision1, precision2], test_files)\n",
    "\n",
    "print \"Score by maj: %f\" % accuracy_score(test_labels, y_test_pred)\n",
    "print \"Score by sum: %f\" % accuracy_score(test_labels, y_test_pred_sum)\n",
    "#print \"Score by prod: %f\" % accuracy_score(test_labels, y_test_pred_prod)\n",
    "\n",
    "#np.savetxt('y_test_pred_mfcc_mlp.txt', y_test_pred, fmt='%d')\n",
    "#np.savetxt('y_test_pred_mfcc_mlp_sum.txt', y_test_pred_sum, fmt='%d')\n",
    "#np.savetxt('y_test_pred_mfcc_mlp_prod.txt', y_test_pred_prod, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
